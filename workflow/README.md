# Workflow that was used to generate this code.

----------------------------

### AI Tools used
  - ChatGPT
     - Use for planning, paperwork, and mockups
  - Claude Code
     - Used for development, coding, and unit testing

### Developer Tools
  - VSCode for Integrated Development Environment (IDE)
  - "Claude Code for VS Code" VSCode extension

This document will cover my interactions with AI to build out this
tool.  It will consist of which AI tools I used for which parts along
with which prompts I passed to the AI.

----------------------------

### Setup a virtual environment to work on
  * Python Virtual environment
    * Setup
         ```bash
         python -m venv llf_venv
         ```
    * Start
         ```bash
         source llf_venv/bin/activate
         ```
    * Check which environment you are in
         ```bash
         echo $VIRTUAL_ENV
         ```
    * Stop
         ```bash
         deactivate
         ```

  * Install python packages
    * pip install -r requirements.txt

----------------------------

### Below are the workflow that I went through to create a Local LLM framework


#### INITIALIZE APPLICATION REPO
1.  Create a new repo, download the repo, and create a new branch to work in
2.  Used ChatGPT to create a Product Requirements Document
      * [1_Create_PRD.txt](1_Create_PRD.txt)
3.  Generated Product Requirements Document generated by ChatGPT and
    given via copy/paste method to Claude for initial coding
      * [PRD.pdf](PRD.pdf)
4.  Create initial directories with a README.md in them based on
    directory structure in the PRD.pdf document
5.  --  COMITTED THE CODE AND CALLED IT PRD DEFINITION COMPLETE


#### NOTES TO REMEMBER FOR FUTURE DEV CONSIDERATIONS
1.  Future-Proofing Requirements (Non-Functional)
      a. Even though the following are out of scope, the code must not block them:
           1.  Multiple LLMs via config file
           2.  API-based access
           3.  GUI frontend
           4.  Voice input/output
           5.  Tool execution (commands, filesystem, internet)
           6.  Permission-based tool access
      b. Key Rule:
           1.  No logic should be tightly coupled to CLI-only assumptions.
2.  Future questions that will need to be answered
      a.  Should model downloads be version-pinned or floating?
      b.  Should inference parameters be user-adjustable in CLI?
      c.  Should conversations be stateful or stateless?
      d.  Should streaming responses be supported in CLI?
      e.  Should this eventually be packaged as a pip-installable tool?


#### INITIAL DEVELOPMENT
1.  Setup virtual environment ( python -m venv llf_venv )
2.  Copy/Pasted PRD document into Claude Code for initial coding
3.  Commands to use:
      a.  Unit testing
            *  pytest
            *  pytest --cov=.
      b.  Download a model
            *  python -m llf.cli download
      c.  Start chatting
            *  python -m llf.cli   
4.  Work on troubleshooting any issues and making sure unit testing
    is working
5.  Got a command line tool called "bin/llf" working with the 
    following ability
      a.  Configurable via a llf/config.py file
      b.  Set a default LLM of Qwen/Qwen3-Coder-30B-A3B-Instruct
      c.  Download LLM's to a local "models" directory
      d.  Provide online help with -h
      e.  Download LLM's from huggingface
           a.  You can specify a specific model or use default
      f.  List the downloaded LLM models
      g.  Show info for the specific models
      h.  Start the local LLM server using the LLM you specify
      i.  Perform chat with LLM via the client program
      j.  If you start the chat program, and the server is not
          started, then it will start it
6.  Found a painful bug.  Come to find out a Macbook can not
    properly run vllm to host an LLM model.  To use vllm, I would
    need to be running Linux on an Intel platfrom.  So I had to
    take the following steps
      a.  I had to use the Llama server code in the following
          repo to host the LLM so I can use the OpenAI library
          interface
             - https://github.com/ggml-org/llama.cpp
      b.  Here are the steps I performed to get the server working
             -  cd llama.cpp
             -  mkdir build
             -  cd build
             -  cmake ..
             -  cmake --build . --config Release
      c.  I wrote the following script to start the server:
             - test_server.sh
      d.  I wrote the following test client script:
             - test_client.py
      e.  Have to use LLM's in a GGUF format for the Llama server
             - You can download the GGUF version from huggingface
             - See if you can convert existing to GGUF
7.  Need to have Claude completely rework the server side code to
    accept parameters in a config file that satisfies the requirements
    of the test_server.sh script.  Technically having a simple shell 
    script would completely acceptable for the server.  Also need to
    update the "llf" command to take this major server change into 
    account.
8.  We want to have a basic working server and basic working client
    before considering the branch complete along with working unit 
    tests.
9.  --  COMMITED THE CODE AND CALLED THE TWEAKS AND CLEANUP COMPLETE


#### Enable the ability to paste a multi-line document as user input
1.  Wnat to be able to copy the content of a PDF and paste it into the user
    input window.
2.  I sent the following prompt to Claude to try to get the multi-line
    input working
       -  2_Multi_Line_Input_CLI.txt
3.  Need to remember to remind Claude to add unit tests
4.  Modified code to enable streaming of text from the LLM instead of
    posting it all when complete
5.  Want to be able to send a request via a command line parameter instead
    of going into interactive mode.  I will create use the following prompt:
       -  3_Input_as_Parm.txt
6.  Add logging ability
7.  Fix issues with server shutting down in daemon mode after the chat is 
    complete
8.  --  COMMITED THE CODE AND CALLED THE TWEAKS AND CLEANUP COMPLETE


#### Cleanup text based interaction and see if we can pip questions to CLI
1.  Want to enable the ability to pipe data into the LLM and get a response.
    This would be an enhancement to the llf chat --cli option
2.  Put configs in a standalone configuration file that I can go in and
    manually update
3.  Get the CLI to work with external LLM's, such as ChatGPT
4.  Update documentation and perform some basic code cleanup
5.  Work with ChatGPT to generate some mockup ideas for how the interactive
    text interface should look like.  This is PURELY to get some ideas.
    Here was the prompt that I used along with a screen shot of the current
    interface:
       -  4_Create_Mockup.txt
     NOTE:  Here is the generated image from ChatGPT
       -  ChatGPT_image.png
6.  Provided the generated screenshot to Claude with a prompt to make the 
    appropriate updates.
7.  Test it all and make sure everything works and that unit testing is 
    complete
8.  All configurations are stored in a config.json file
9.  --  COMMITED THE CODE AND CALLED THE TWEAKS AND CLEANUP COMPLETE


#### Added a bunch of misc functionality
1.  Enable the app to not require local_llm_server entry in the config.json
    file if you are using an external LLM
2.  Modify the config.json to be able to support both HuggingFace and GGUF
    formatted LLM's
3.  Created example config.json files to use as references for different 
    configurations
4.  Created a script called convert_huggingface_llm_2_gguf.sh that will convert
    a downloaded HuggingFace LLM to a GGUF image so it can be ran from the
    internal Llama server
5.  Created a config_prompt.json file that can optionally be used to pass 
    different types of prompts to the LLM with your request, such as System, 
    Master, and Assistant prompts.
6.  A number of misc tweaks
7.  --  COMMITED THE CODE AND CALLED THE TWEAKS AND CLEANUP COMPLETE


#### Create an optional GUI frontend
1.  Create a GUI frontend that can be enabled and started with the "llf gui enable"
    option.  I passed the following prompt for Claude to do the coding:
       -  5_GUI.txt
2.  It provided a GUI that can be accessed via a web browser.  Adding some minor 
    tweaks to the web based GUI.  All functionality I have in the CLI I want in
    the GUI.  Including the ability to modify the .json config files
3.  You can pass in optional server parameters to the locally running Llama server
    via additional "server_params" added to the "local_llm_server" in the 
    config.json file.  These parameters are 100% optional 
4.  Moving configuration files and their associated examples to a configs directory
5.  Creatinga  backup location in the configs directory to backup config files when
    needed
6.  Add the ability in the GUI to backup the configuration files 
7.  Add the ability to enable the llama server on the network interface so that
    you can enable people on your local network access to your running LLM
      - The default will be to only run on localhost 127.0.0.1
      - This is done via the --share parameter
8.  Add the ability to run the GUI interface through the network interface so 
    that you can enable people on your local network access to your GUI
    interface. 
      - The default will be to only run on localhost 127.0.0.1
      - This is done via the --share parameter
9.  Add an optional "secret key" as a parameter passed to the "llf gui" command
    that is used to require a simple login access to the GUI.  The user just
    needs to type in the same value as the "secret key" to get access to the
    GUI interface.  This is to provide some form of basic security when you
    decide to grant access to the GUI on your private network.
10. Add the following options for the GUI options
      - llf gui start 
         - This will do the same thing as "llf gui".  This is just needed
           for standardization
      - llf gui start --daemon
         - This will start the GUI as a daemon process similar to how the
           LLM ( llama ) server operates with the --daemon parameter
      - llf gui stop
         - Stop the GUI process if it is running
11. Fix misc bugs and isseus with the GUI
12.  --  COMMITED THE CODE AND CALLED THE TWEAKS AND CLEANUP COMPLETE


#### Create a basic framework for extended compontents
1.  Create a section to work with memory  
       - Create "llf datastore"  ( Call it "Data Store Management" )
       - This section will be for custom memory sources used by the
         LLM for data lookups, such as a RAG data source
2.  Create a section to work with modules
       - Create  "llf module"  ( Call it "Module Management" )
       - This section will be for modules that extends the LLM 
         engagement with the end user, such as enabling text to
         speech and visa versa
3.  Create a section to work with tools
       - Create  "llf tool"  ( Call it "Tool Management" )
       - This section will be for tools that extends the LLM
         capability, such as enabling internet search access and
         the ability to access files locally on your computer
12.  --  COMMITED THE CODE AND CALLED THE TWEAKS AND CLEANUP COMPLETE


#### Perform major testing and resolve all bugs and update docs
1.  Ensure all unit tests are in place for code coverage and works
2.  Verify all documentation is up to data accordingly
3.  Perform a detailed code review and see if there are any performance 
    improvements that should be made.  Remember to perform all this work
    in the virtual environment
4.  Perform a detailed code review and see if there is any cluttered 
    code that needs to be reworked and better stream lined.  Is there
    any coding optimization that can take place that will not affected
    the intended functionality of the code.  Remember to perform all
    this work in the virtual environment and verify with unit testing
5.  Perform a final code review and add any comments in the code to
    make the code easier to read, understand, and work on by future
    developers
6.  Perform detailed testing.  Refer to the following testing outline:
       -  6_Testing_Outline.txt
7.  More testing     
8.  --  COMMITED THE CODE AND CALLED THE TWEAKS AND CLEANUP COMPLETE


#### Work on modules ( text to speech AND speech to text )
1.  Start development of a text to speech module that will be used
    to take text generated from the LLM and play it as audio along with
    print the text on the screen.
2.  Module location for the text to speech module:
       -  modules/text2speech
3.  Used the following prompt to being the development of the
    text to speech module:
       -  7_Text_to_Speech.txt
4.  Worked through integration issues so it would work via the CLI and GUI
5.  Created a Speech To Text module to be used in interactive Terminal chat
    and GUI
6.  Work through small integration issues
7.  --  COMMITED THE CODE AND CALLED THE TWEAKS AND CLEANUP COMPLETE


#### Fix all unit tests and improve code coverage
1.  Work on fixing unit test issues
2.  Increase code coverage 
3.  Work through issues     
4.  --  COMMITED THE CODE AND CALLED THE TWEAKS AND CLEANUP COMPLETE


#### Work on custom memory utilizing RAG
1.  Built out the "data_store" directory structure to be used to store RAG 
    data.
2.  Built a number of tooling scripts that are in the "data_store/tools" directory
    and have them do the following:
       - Read the following type of documentation and convert it to a .JSONL format
         that can be loaded into a RAG data store
            - PDF files
            - Microsoft Word files
            - TXT files
            - Web site text data
       - Validation tool to verify the .JSONL files are good for data loading
       - Create a RAG Vector Store and load the .JSONL files into them
3.  Created a data_store_registry.json file that will be used similarly as the 
    modules_registry.json file in the "modules" directory
4.  Used the following prompt to being development of the "llf datastore" code
       -  8_Data_Store.txt
5.  Do some basic troubleshooting and verifying the changes are working
6.  Get the GUI management for the Data Stores working
7.  Verify and do basic troublshooting and unit testing
8.  Add the logic to actually use the Data Stores by the LLM's
9.  Verified it work via the CLI and GUI interface.  Did preliminary testing and
    troubleshoot any issues that stands out
10.  --  COMMITED THE CODE AND CALLED THE TWEAKS AND CLEANUP COMPLETE


#### Add Long term memory that the LLM can use
1.  Create directory structure, configuration, and management commands
2.  Setup management tools for the memory module
       -  9_Memory_Commands.txt
3.  Verify code works and do basic troubleshooting and testing
4.  Start integrating long term memory system
5.  Test the memory and perform any needed troubleshooting and unit testing
6.  Found an interesting issue where the streaming data is breaking the memory usage
      - Memory usage operates like a "tool"
      - Added logic to not stream the LLM response if memory is being used
      - Noticed the LLM model I was using was using XML instead of JSON
         - Creating am "llf tool" command to enable/disable XML usage
             CONVERT TOOL DATA TO XML:  llf tool xml_format enable
             USE JSON:  llf tool xml_format disable  
7.  Made minor tweaks and changes that will need to be documented later
8.  Doing a number of minor code tweaks and testing.
9.  Since it wasn't a massive code change, added the ability to have multiple
    server configurations and the ability to manage them
10.  --  COMMITED THE CODE AND CALLED THE TWEAKS AND CLEANUP COMPLETE


#### Add proper documentation and perform detailed testing
1.  Perform another round of detailed testing and create documentation in
    the process.
2.  Update README.md and have it reference documentation in the "docs" directory
3.  Moved all the tool scripts to a single location under bin
4.  Had Claude create a tool for me and used the following prompt
       -  10_Tool_script.txt
5.  Had Claude create/modify some tool testing scripts with the following prompt:
       - 11_More_Tools.txt
6.  Here is one of the prompts I gave to Claude to create documentation that
    I then went in and modified.  I used AI the create the initial draft that I 
    then cleaned up.
       - 12_Documentation.txt
7.  From going through the documentation, I came up with a few minor enhancements
    that I added.  Such as the following:
       - Have the tool create and delete long term memory modules so you don't have to
         run an external script
       - Have the tool import and export the Data Stores ( RAG Vector Stores ) which
         will handle all the updates to the registry file so you don't have to.        

#### Develop different tools that can be enabled and disabled
1.  Explain to Claude what we are going to do and see if there is a common coding interface
    it should setup to enable the LLM to work with different tools.  This is so we can 
    easily create tools to perform different tasks that can easily be plugged into the LLM
    for use.
       - 13_Tool_Integration_Design.txt
2.  Worked on building out a number of different tools, such as the following
       command_exec - Enable LLM to run commands locally on computer
          - This is controlled via whitelisting commands that the LLM can execute
       file_access - Enable LLM to view and updated files on local computer
          - This is controlled via whitelisting files and directories the LLM can access
       internet_duckduckgo - Enable LLM to use DuckDuckGo Internet search access
       internet_google_webscrape - Enable LLM to use Google Internet search for free
       internet_google_api - Enable LLM to use the official Google API for search
          - This requires the purchase of tokens to use and an API key
3.  Add documentation about using and developing tools and how different LLF workflows operate
4.  Review code base of any possible coding enhancements
5.  Review unit testing
6.  Ask the AI to review the code and see if there is any functionality that it would recommend
    adding and explain what it would do and how it would benefit the application and end user
        - This information will be collected and reviewed.  It may get acted upon in 
          the following section.

#### Review recommendations from the AI.
1.  The following is the list of recommendations from the AI regarding what should be added or
    changed and why.
       - 14_Input_From_AI.txt
2.  This is what I like from the AI recommended list
      Category 1: User Experience Enhancements 
      3. Add Progress Indicators for Long Operations
          - I like the ability to provide status information that is taking place
      4. Add Conversation Export with Multiple Formats
          - This would go hand in hand with 5.2 ( Conversation Export and Import )
      5. Implement Soft-Delete with Trash System
          - I like this for the memories and datastores
          - Models are too big to do with with

      Category 2: Developer Experience Improvements
      1. Add Tool Development Scaffolding System
          - I like this.  Creates framework to build additional tools
      2. Add Debug Mode with Detailed Logging
          - I like this

      Category 3: Architecture & Performance 
      2. Add Caching Layer for Embeddings and Queries
          - I like the possible performance gains from ths
      4. Add Async/Await Support for Parallel Tool Execution
          - I like the possible performance gains from ths
      5. Implment Response Streaming with Server-Sent Events (SSE)
          - I like this, but this might mess with the voice to text and text to voice
            functionality.  Would need to do some testing

      Category 4: Security Hardening
      1. Implement Tool Sandboxing with Process Isolation
          - I like this but maybe not right now
      2. Add API Key Encryption and Secure Storage
          - I like this but maybe not right now
      5. Add Audit Logging for Security Events
          - I like this for tool usage

      Category 5: Feature Additions 
      2. Add Conversation Export and Import
          - This would go hand in hand with 1.4 ( Conversation Export with Multiple Formats )
      4. Add Prompt Templates and Management
          - I like this a lot
      6. Add Conversation Analytics and Insights
          - I like this but maybe not right now

      Category 6: Documentation & Quality 
      1. Add Comprehensive API Documentation
           - Lets add this

#### Testing all the functionality to verify everything seems to work as expected







# Workflow that was used to generate this code.

----------------------------

### AI Tools used
  - ChatGPT
     - Use for planning, paperwork, and mockups
  - Claude Code
     - Used for development, coding, and unit testing

### Developer Tools
  - VSCode for Integrated Development Environment (IDE)
  - "Claude Code for VS Code" VSCode extension

This document will cover my interactions with AI to build out this
tool.  It will consist of which AI tools I used for which parts along
with which prompts I passed to the AI.

----------------------------

### Setup a virtual environment to work on
  * Python Virtual environment
    * Setup
         ```bash
         python -m venv llf_venv
         ```
    * Start
         ```bash
         source llf_venv/bin/activate
         ```
    * Check which environment you are in
         ```bash
         echo $VIRTUAL_ENV
         ```
    * Stop
         ```bash
         deactivate
         ```

  * Install python packages
    * pip install -r requirements.txt

----------------------------

### Below are the workflow that I went through to create the Pong Game

#### INITIALIZE APPLICATION REPO
1.  Create a new repo, download the repo, and create a new branch to work in
2.  Used ChatGPT to create a Product Requirements Document
      * [1_Create_PRD.txt](1_Create_PRD.txt)
3.  Generated Product Requirements Document generated by ChatGPT and
    given via copy/paste method to Claude for initial coding
      * [PRD.pdf](PRD.pdf)
4.  Create initial directories with a README.md in them based on
    directory structure in the PRD.pdf document
5.  --  COMITTED THE CODE AND CALLED IT PRD DEFINITION COMPLETE

#### NOTES TO REMEMBER FOR FUTURE DEV CONSIDERATIONS
1.  Future-Proofing Requirements (Non-Functional)
      a. Even though the following are out of scope, the code must not block them:
           1.  Multiple LLMs via config file
           2.  API-based access
           3.  GUI frontend
           4.  Voice input/output
           5.  Tool execution (commands, filesystem, internet)
           6.  Permission-based tool access
      b. Key Rule:
           1.  No logic should be tightly coupled to CLI-only assumptions.
2.  Future questions that will need to be answered
      a.  Should model downloads be version-pinned or floating?
      b.  Should inference parameters be user-adjustable in CLI?
      c.  Should conversations be stateful or stateless?
      d.  Should streaming responses be supported in CLI?
      e.  Should this eventually be packaged as a pip-installable tool?

#### INITIAL DEVELOPMENT
1.  Setup virtual environment ( python -m venv llf_venv )
2.  Copy/Pasted PRD document into Claude Code for initial coding
3.  Commands to use:
      a.  Unit testing
            *  pytest
            *  pytest --cov=.
      b.  Download a model
            *  python -m llf.cli download
      c.  Start chatting
            *  python -m llf.cli   
4.  Work on troubleshooting any issues and making sure unit testing
    is working
5.  Got a command line tool called "bin/llf" working with the 
    following ability
      a.  Configurable via a llf/config.py file
      b.  Set a default LLM of Qwen/Qwen3-Coder-30B-A3B-Instruct
      c.  Download LLM's to a local "models" directory
      d.  Provide online help with -h
      e.  Download LLM's from huggingface
           a.  You can specify a specific model or use default
      f.  List the downloaded LLM models
      g.  Show info for the specific models
      h.  Start the local LLM server using the LLM you specify
      i.  Perform chat with LLM via the client program
      j.  If you start the chat program, and the server is not
          started, then it will start it



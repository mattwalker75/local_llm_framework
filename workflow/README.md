# Workflow that was used to generate this code.

----------------------------

### AI Tools used
  - ChatGPT
     - Use for planning, paperwork, and mockups
  - Claude Code
     - Used for development, coding, and unit testing

### Developer Tools
  - VSCode for Integrated Development Environment (IDE)
  - "Claude Code for VS Code" VSCode extension

This document will cover my interactions with AI to build out this
tool.  It will consist of which AI tools I used for which parts along
with which prompts I passed to the AI.

----------------------------

### Setup a virtual environment to work on
  * Python Virtual environment
    * Setup
         ```bash
         python -m venv llf_venv
         ```
    * Start
         ```bash
         source llf_venv/bin/activate
         ```
    * Check which environment you are in
         ```bash
         echo $VIRTUAL_ENV
         ```
    * Stop
         ```bash
         deactivate
         ```

----------------------------

### Below are the workflow that I went through to create the Pong Game

#### INITIALIZE APPLICATION REPO
1.  Create a new repo, download the repo, and create a new branch to work in
2.  Used ChatGPT to create a Product Requirements Document
      * [1_Create_PRD.txt](1_Create_PRD.txt)
3.  Generated Product Requirements Document generated by ChatGPT and
    given via copy/paste method to Claude for initial coding
      * [PRD.pdf](PRD.pdf)
4.  Create initial directories with a README.md in them based on
    directory structure in the PRD.pdf document
5.  --  COMITTED THE CODE AND CALLED IT PRD DEFINITION COMPLETE

#### NOTES TO REMEMBER FOR FUTURE DEV CONSIDERATIONS
1.  Future-Proofing Requirements (Non-Functional)
      a. Even though the following are out of scope, the code must not block them:
           1.  Multiple LLMs via config file
           2.  API-based access
           3.  GUI frontend
           4.  Voice input/output
           5.  Tool execution (commands, filesystem, internet)
           6.  Permission-based tool access
      b. Key Rule:
           1.  No logic should be tightly coupled to CLI-only assumptions.
2.  Future questions that will need to be answered
      a.  Should model downloads be version-pinned or floating?
      b.  Should inference parameters be user-adjustable in CLI?
      c.  Should conversations be stateful or stateless?
      d.  Should streaming responses be supported in CLI?
      e.  Should this eventually be packaged as a pip-installable tool?



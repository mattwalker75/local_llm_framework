# Workflow that was used to generate this code.

----------------------------

### AI Tools used
  - ChatGPT
     - Use for planning, paperwork, and mockups
  - Claude Code
     - Used for development, coding, and unit testing

### Developer Tools
  - VSCode for Integrated Development Environment (IDE)
  - "Claude Code for VS Code" VSCode extension

This document will cover my interactions with AI to build out this
tool.  It will consist of which AI tools I used for which parts along
with which prompts I passed to the AI.

----------------------------

### Setup a virtual environment to work on
  * Python Virtual environment
    * Setup
         ```bash
         python -m venv llf_venv
         ```
    * Start
         ```bash
         source llf_venv/bin/activate
         ```
    * Check which environment you are in
         ```bash
         echo $VIRTUAL_ENV
         ```
    * Stop
         ```bash
         deactivate
         ```

  * Install python packages
    * pip install -r requirements.txt

----------------------------

### Below are the workflow that I went through to create a Local LLM framework

#### INITIALIZE APPLICATION REPO
1.  Create a new repo, download the repo, and create a new branch to work in
2.  Used ChatGPT to create a Product Requirements Document
      * [1_Create_PRD.txt](1_Create_PRD.txt)
3.  Generated Product Requirements Document generated by ChatGPT and
    given via copy/paste method to Claude for initial coding
      * [PRD.pdf](PRD.pdf)
4.  Create initial directories with a README.md in them based on
    directory structure in the PRD.pdf document
5.  --  COMITTED THE CODE AND CALLED IT PRD DEFINITION COMPLETE

#### NOTES TO REMEMBER FOR FUTURE DEV CONSIDERATIONS
1.  Future-Proofing Requirements (Non-Functional)
      a. Even though the following are out of scope, the code must not block them:
           1.  Multiple LLMs via config file
           2.  API-based access
           3.  GUI frontend
           4.  Voice input/output
           5.  Tool execution (commands, filesystem, internet)
           6.  Permission-based tool access
      b. Key Rule:
           1.  No logic should be tightly coupled to CLI-only assumptions.
2.  Future questions that will need to be answered
      a.  Should model downloads be version-pinned or floating?
      b.  Should inference parameters be user-adjustable in CLI?
      c.  Should conversations be stateful or stateless?
      d.  Should streaming responses be supported in CLI?
      e.  Should this eventually be packaged as a pip-installable tool?

#### INITIAL DEVELOPMENT
1.  Setup virtual environment ( python -m venv llf_venv )
2.  Copy/Pasted PRD document into Claude Code for initial coding
3.  Commands to use:
      a.  Unit testing
            *  pytest
            *  pytest --cov=.
      b.  Download a model
            *  python -m llf.cli download
      c.  Start chatting
            *  python -m llf.cli   
4.  Work on troubleshooting any issues and making sure unit testing
    is working
5.  Got a command line tool called "bin/llf" working with the 
    following ability
      a.  Configurable via a llf/config.py file
      b.  Set a default LLM of Qwen/Qwen3-Coder-30B-A3B-Instruct
      c.  Download LLM's to a local "models" directory
      d.  Provide online help with -h
      e.  Download LLM's from huggingface
           a.  You can specify a specific model or use default
      f.  List the downloaded LLM models
      g.  Show info for the specific models
      h.  Start the local LLM server using the LLM you specify
      i.  Perform chat with LLM via the client program
      j.  If you start the chat program, and the server is not
          started, then it will start it
6.  Found a painful bug.  Come to find out a Macbook can not
    properly run vllm to host an LLM model.  To use vllm, I would
    need to be running Linux on an Intel platfrom.  So I had to
    take the following steps
      a.  I had to use the Llama server code in the following
          repo to host the LLM so I can use the OpenAI library
          interface
             - https://github.com/ggml-org/llama.cpp
      b.  Here are the steps I performed to get the server working
             -  cd llama.cpp
             -  mkdir build
             -  cd build
             -  cmake ..
             -  cmake --build . --config Release
      c.  I wrote the following script to start the server:
             - test_server.sh
      d.  I wrote the following test client script:
             - test_client.py
      e.  Have to use LLM's in a GGUF format for the Llama server
             - You can download the GGUF version from huggingface
             - See if you can convert existing to GGUF
7.  Need to have Claude completely rework the server side code to
    accept parameters in a config file that satisfies the requirements
    of the test_server.sh script.  Technically having a simple shell 
    script would completely acceptable for the server.  Also need to
    update the "llf" command to take this major server change into 
    account.
8.  We want to have a basic working server and basic working client
    before considering the branch complete along with working unit 
    tests.
9.  --  COMMITED THE CODE AND CALLED THE TWEAKS AND CLEANUP COMPLETE

#### Enable the ability to paste a multi-line document as user input
1.  Wnat to be able to copy the content of a PDF and paste it into the user
    input window.
2.  I sent the following prompt to Claude to try to get the multi-line
    input working
       -  2_Multi_Line_Input_CLI.txt
3.  Need to remember to remind Claude to add unit tests
4.  Modified code to enable streaming of text from the LLM instead of
    posting it all when complete
5.  Want to be able to send a request via a command line parameter instead
    of going into interactive mode.  I will create use the following prompt:
       -  3_Input_as_Parm.txt
6.  Add logging ability
7.  Fix issues with server shutting down in daemon mode after the chat is 
    complete
8.  --  COMMITED THE CODE AND CALLED THE TWEAKS AND CLEANUP COMPLETE

#### Cleanup text based interaction and see if we can pip questions to CLI
1.  Want to enable the ability to pipe data into the LLM and get a response.
    This would be an enhancement to the llf chat --cli option
2.  Put configs in a standalone configuration file that I can go in and
    manually update
3.  Get the CLI to work with external LLM's, such as ChatGPT
4.  Update documentation and perform some basic code cleanup
5.  Work with ChatGPT to generate some mockup ideas for how the interactive
    text interface should look like.  This is PURELY to get some ideas.
    Here was the prompt that I used along with a screen shot of the current
    interface:
       -  4_Create_Mockup.txt
     NOTE:  Here is the generated image from ChatGPT
       -  ChatGPT_image.png
6.  Provided the generated screenshot to Claude with a prompt to make the 
    appropriate updates.
7.  Test it all and make sure everything works and that unit testing is 
    complete
8.  All configurations are stored in a config.json file
9.  --  COMMITED THE CODE AND CALLED THE TWEAKS AND CLEANUP COMPLETE

#### Added a bunch of misc functionality
1.  Enable the app to not require local_llm_server entry in the config.json
    file if you are using an external LLM
2.  Modify the config.json to be able to support both HuggingFace and GGUF
    formatted LLM's
3.  Created example config.json files to use as references for different 
    configurations
4.  Created a script called convert_huggingface_llm_2_gguf.sh that will convert
    a downloaded HuggingFace LLM to a GGUF image so it can be ran from the
    internal Llama server
5.  Created a config_prompt.json file that can optionally be used to pass 
    different types of prompts to the LLM with your request, such as System, 
    Master, and Assistant prompts.
6.  A number of misc tweaks
7.  --  COMMITED THE CODE AND CALLED THE TWEAKS AND CLEANUP COMPLETE

#### Create an optional GUI frontend
1.  Create a GUI frontend that can be enabled and started with the "llf gui enable"
    option.  I passed the following prompt for Claude to do the coding:
       -  5_GUI.txt
2.  It provided a GUI that can be accessed via a web browser.  Adding some minor 
    tweaks to the web based GUI.  All functionality I have in the CLI I want in
    the GUI.  Including the ability to modify the .json config files
3.  You can pass in optional server parameters to the locally running Llama server
    via additional "server_params" added to the "local_llm_server" in the 
    config.json file.  These parameters are 100% optional 
4.  Moving configuration files and their associated examples to a configs directory
5.  Creatinga  backup location in the configs directory to backup config files when
    needed
6.  Add the ability in the GUI to backup the configuration files 
7.  Add the ability to enable the llama server on the network interface so that
    you can enable people on your local network access to your running LLM
      - The default will be to only run on localhost 127.0.0.1
      - This is done via the --share parameter
8.  Add the ability to run the GUI interface through the network interface so 
    that you can enable people on your local network access to your GUI
    interface. 
      - The default will be to only run on localhost 127.0.0.1
      - This is done via the --share parameter
9.  Add an optional "secret key" as a parameter passed to the "llf gui" command
    that is used to require a simple login access to the GUI.  The user just
    needs to type in the same value as the "secret key" to get access to the
    GUI interface.  This is to provide some form of basic security when you
    decide to grant access to the GUI on your private network.
10. Add the following options for the GUI options
      - llf gui start 
         - This will do the same thing as "llf gui".  This is just needed
           for standardization
      - llf gui start --daemon
         - This will start the GUI as a daemon process similar to how the
           LLM ( llama ) server operates with the --daemon parameter
      - llf gui stop
         - Stop the GUI process if it is running
11. Fix misc bugs and isseus with the GUI
12.  --  COMMITED THE CODE AND CALLED THE TWEAKS AND CLEANUP COMPLETE

#### Create a basic framework for extended compontents
1.  Create a section to work with memory  
       - Create "llf datastore"  ( Call it "Data Store Management" )
       - This section will be for custom memory sources used by the
         LLM for data lookups, such as a RAG data source
2.  Create a section to work with modules
       - Create  "llf module"  ( Call it "Module Management" )
       - This section will be for modules that extends the LLM 
         engagement with the end user, such as enabling text to
         speech and visa versa
3.  Create a section to work with tools
       - Create  "llf tool"  ( Call it "Tool Management" )
       - This section will be for tools that extends the LLM
         capability, such as enabling internet search access and
         the ability to access files locally on your computer
12.  --  COMMITED THE CODE AND CALLED THE TWEAKS AND CLEANUP COMPLETE

#### Perform major testing and resolve all bugs and update docs




     

      


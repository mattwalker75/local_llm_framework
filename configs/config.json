{
  "local_llm_server": {
    "llama_server_path": "../llama.cpp/build/bin/llama-server",
    "server_host": "127.0.0.1",
    "server_port": 8000,
    "model_dir": "Qwen--Qwen2.5-Coder-7B-Instruct-GGUF",
    "gguf_file": "qwen2.5-coder-7b-instruct-q5_k_m.gguf",
    "server_params": {
      "_comment": "Additional llama-server parameters (optional). Run '../llama.cpp/build/bin/llama-server -h' to see all options.",
      "_examples": "Uncomment and modify the parameters below as needed:",
      "_ctx-size": 2048,
      "_ctx-size_note": "Context window size (default varies by model, typically 512-4096)",
      "_n-gpu-layers": 0,
      "_n-gpu-layers_note": "GPU layers to offload (0=CPU only, -1=all layers, or specific number)",
      "_threads": 4,
      "_threads_note": "CPU threads for inference (default is typically 4-8)",
      "_batch-size": 512,
      "_batch-size_note": "Batch size for prompt processing",
      "_flash-attn": false,
      "_flash-attn_note": "Enable flash attention for faster processing (if supported by model)"
    }
  },
  "llm_endpoint": {
    "api_base_url": "http://127.0.0.1:8000/v1",
    "api_key": "EMPTY",
    "model_name": "Qwen/Qwen2.5-Coder-7B-Instruct-GGUF"
  },
  "model_dir": "models",
  "cache_dir": ".cache",
  "log_level": "ERROR",
  "inference_params": {
    "temperature": 0.7,
    "max_tokens": 2048,
    "top_p": 0.9,
    "top_k": 50,
    "repetition_penalty": 1.1
  }
}

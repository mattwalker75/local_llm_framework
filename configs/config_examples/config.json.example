{
  "_comment": "LLF Configuration File - Copy this to config.json and customize",

  "_local_llm_server_comment": "Local server settings (server-side, only used when starting local llama-server)",
  "local_llm_server": {
    "llama_server_path": "../llama.cpp/build/bin/llama-server",
    "server_host": "127.0.0.1",
    "server_port": 8000,
    "gguf_file": "qwen2.5-coder-7b-instruct-q4_k_m.gguf"
  },

  "_llm_endpoint_comment": "LLM API endpoint (client-side, change these to switch between local and external LLMs)",
  "_switching_examples": {
    "_local_example": "For local: api_base_url='http://127.0.0.1:8000/v1', model_name='Qwen/Qwen2.5-Coder-7B-Instruct-GGUF', api_key='EMPTY'",
    "_openai_example": "For OpenAI: api_base_url='https://api.openai.com/v1', model_name='gpt-4', api_key='sk-proj-YOUR-KEY'",
    "_anthropic_example": "For Anthropic: api_base_url='https://api.anthropic.com/v1', model_name='claude-3-opus-20240229', api_key='sk-ant-YOUR-KEY'"
  },
  "llm_endpoint": {
    "api_base_url": "http://127.0.0.1:8000/v1",
    "api_key": "EMPTY",
    "model_name": "Qwen/Qwen2.5-Coder-7B-Instruct-GGUF"
  },

  "_paths_comment": "Paths can be absolute or relative to project root",
  "model_dir": "models",
  "cache_dir": ".cache",

  "_logging_comment": "Valid log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL",
  "log_level": "ERROR",

  "_inference_comment": "LLM inference parameters",
  "inference_params": {
    "temperature": 0.7,
    "max_tokens": 2048,
    "top_p": 0.9,
    "top_k": 50,
    "repetition_penalty": 1.1
  }
}

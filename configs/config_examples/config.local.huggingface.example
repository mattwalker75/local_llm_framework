{
  "_comment": "===== LLF Configuration for Local LLM with HuggingFace Model Structure =====",
  "_usage": "To use this config: cp config.local.huggingface.example config.json",

  "_setup_instructions": {
    "step1": "Ensure llama.cpp is built (see README for instructions)",
    "step2": "Copy this file to config.json",
    "step3": "Download a model from HuggingFace: llf model download",
    "step4": "(Optional) Update model_name below if using a different model",
    "step5": "Start chatting: llf chat"
  },

  "_use_case": "Use this config when downloading models from HuggingFace Hub (recommended for most users)",

  "_local_llm_server_comment": "===== Local llama-server Configuration =====",
  "_local_llm_server_note": "These settings control the local llama-server process when LLF starts it",
  "local_llm_server": {
    "llama_server_path": "../llama.cpp/build/bin/llama-server",
    "_llama_server_path_note": "Path to llama-server binary (relative to project root)",

    "server_host": "127.0.0.1",
    "_server_host_note": "127.0.0.1 = localhost only, 0.0.0.0 = accessible on network",

    "server_port": 8000,
    "_server_port_note": "Port for llama-server to listen on",

    "_model_location_comment": "===== HuggingFace Model Location =====",
    "_model_location_note": "Models downloaded from HuggingFace containing safetensors files",

    "model_dir": "Qwen--Qwen2.5-Coder-7B-Instruct",
    "_model_dir_note": "HuggingFace model directory in models/ (use -- instead of / from model name)",
    "_model_dir_example": "For 'Qwen/Qwen2.5-Coder-7B-Instruct' use 'Qwen--Qwen2.5-Coder-7B-Instruct'",

    "_important_note": "This config is for safetensors format (standard HuggingFace models)",
    "_path_example": "Full path: models/Qwen--Qwen2.5-Coder-7B-Instruct/ (contains safetensors files)",

    "_server_params_comment": "===== Additional llama-server Parameters (Optional) =====",
    "_server_params_note": "Pass any additional parameters to llama-server. These are passed as command-line arguments.",
    "_server_params_help": "To see all available options, run: ../llama.cpp/build/bin/llama-server -h",
    "server_params": {
      "ctx-size": 8192,
      "_ctx_size_note": "Context window size (default is usually 512-2048, increase for longer conversations)",
      "n-gpu-layers": 35,
      "_n_gpu_layers_note": "Number of layers to offload to GPU (0 = CPU only, -1 = all layers)",
      "threads": 8,
      "_threads_note": "Number of CPU threads to use for inference"
    },
    "_server_params_optional": "The server_params section is completely optional. Remove it to use llama-server defaults.",

    "_popular_models": {
      "qwen_7b": "Qwen/Qwen2.5-Coder-7B-Instruct (recommended, good balance)",
      "llama3_8b": "meta-llama/Meta-Llama-3-8B-Instruct",
      "mistral_7b": "mistralai/Mistral-7B-Instruct-v0.2",
      "codellama_7b": "codellama/CodeLlama-7b-Instruct-hf"
    }
  },

  "_llm_endpoint_comment": "===== LLM Endpoint Configuration =====",
  "_llm_endpoint_note": "Points to local llama-server (matches server_host and server_port above)",
  "llm_endpoint": {
    "api_base_url": "http://127.0.0.1:8000/v1",
    "_api_base_url_note": "OpenAI-compatible API endpoint (local server)",
    "api_key": "EMPTY",
    "_api_key_note": "No authentication needed for local server",
    "model_name": "Qwen/Qwen2.5-Coder-7B-Instruct",
    "_model_name_note": "HuggingFace model repo name (used for downloads, NOT GGUF repos)"
  },

  "_paths_comment": "===== Storage Directories =====",
  "model_dir": "models",
  "_model_dir_note": "Base directory where downloaded models are stored",
  "cache_dir": ".cache",
  "_cache_dir_note": "Cache directory for temporary files",

  "_logging_comment": "===== Logging Configuration =====",
  "_logging_levels": "Options: DEBUG, INFO, WARNING, ERROR, CRITICAL",
  "log_level": "ERROR",

  "_inference_params_comment": "===== Inference Parameters =====",
  "_inference_params_note": "These control how the LLM generates responses",
  "inference_params": {
    "temperature": 0.7,
    "_temperature_note": "Randomness (0.0 = deterministic, 2.0 = very creative)",
    "max_tokens": 2048,
    "_max_tokens_note": "Maximum response length in tokens",
    "top_p": 0.9,
    "_top_p_note": "Nucleus sampling (0.0-1.0, higher = more diversity)",
    "top_k": 50,
    "_top_k_note": "Top-k sampling (llama.cpp only, not supported by OpenAI)",
    "repetition_penalty": 1.1,
    "_repetition_penalty_note": "Penalty for repeating tokens (llama.cpp only, 1.0 = no penalty)"
  },

  "_switching_configs": {
    "to_gguf": "To use GGUF format models: cp config.local.gguf.example config.json",
    "to_openai": "To switch to OpenAI: cp config.openai.example config.json",
    "to_anthropic": "To switch to Anthropic: cp config.anthropic.example config.json",
    "note": "HuggingFace config = safetensors format, GGUF config = GGUF format"
  }
}

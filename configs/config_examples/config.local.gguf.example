{
  "_comment": "===== LLF Configuration for Local LLM with Custom GGUF Directory =====",
  "_usage": "To use this config: cp config.local.gguf.example config.json",

  "_setup_instructions": {
    "step1": "Ensure llama.cpp is built (see README for instructions)",
    "step2": "Copy this file to config.json",
    "step3": "Place your GGUF model file in models/{model_dir}/ directory",
    "step4": "Update model_dir and gguf_file below to match your setup",
    "step5": "Start chatting: llf chat"
  },

  "_use_case": "Use this config when you have GGUF models in a custom directory structure (not from HuggingFace)",

  "_local_llm_server_comment": "===== Local llama-server Configuration =====",
  "_local_llm_server_note": "These settings control the local llama-server process when LLF starts it",
  "local_llm_server": {
    "llama_server_path": "../llama.cpp/build/bin/llama-server",
    "_llama_server_path_note": "Path to llama-server binary (relative to project root)",

    "server_host": "127.0.0.1",
    "_server_host_note": "127.0.0.1 = localhost only, 0.0.0.0 = accessible on network",

    "server_port": 8000,
    "_server_port_note": "Port for llama-server to listen on",

    "_model_location_comment": "===== Custom GGUF Model Location =====",
    "_model_location_note": "Specify a custom subdirectory in models/ for your GGUF file",

    "model_dir": "custom_models",
    "_model_dir_note": "Subdirectory name in models/ (creates path: models/custom_models/)",

    "gguf_file": "my-model.gguf",
    "_gguf_file_note": "GGUF model file name (quantized format for llama.cpp)",

    "_full_path_example": "Full path will be: models/custom_models/my-model.gguf",

    "_server_params_comment": "===== Additional llama-server Parameters (Optional) =====",
    "_server_params_note": "Pass any additional parameters to llama-server. These are passed as command-line arguments.",
    "_server_params_help": "To see all available options, run: ../llama.cpp/build/bin/llama-server -h",
    "server_params": {
      "ctx-size": 8192,
      "_ctx_size_note": "Context window size (default is usually 512-2048, increase for longer conversations)",
      "n-gpu-layers": 35,
      "_n_gpu_layers_note": "Number of layers to offload to GPU (0 = CPU only, -1 = all layers)",
      "threads": 8,
      "_threads_note": "Number of CPU threads to use for inference"
    },
    "_server_params_examples": {
      "example1": "Large context: 'ctx-size': 16384",
      "example2": "GPU acceleration: 'n-gpu-layers': -1 (all layers)",
      "example3": "Flash attention: 'flash-attn': true",
      "example4": "Batch size: 'batch-size': 512"
    },
    "_server_params_optional": "The server_params section is completely optional. Remove it to use llama-server defaults.",

    "_common_use_cases": {
      "example1": "Downloaded GGUF from a URL: model_dir: 'my_downloads', gguf_file: 'llama-2-7b.gguf'",
      "example2": "Custom quantized model: model_dir: 'custom_quants', gguf_file: 'model-q4_k_m.gguf'",
      "example3": "Testing multiple models: model_dir: 'test_models', gguf_file: 'experimental.gguf'"
    }
  },

  "_llm_endpoint_comment": "===== LLM Endpoint Configuration =====",
  "_llm_endpoint_note": "Points to local llama-server (matches server_host and server_port above)",
  "llm_endpoint": {
    "api_base_url": "http://127.0.0.1:8000/v1",
    "_api_base_url_note": "OpenAI-compatible API endpoint (local server)",
    "api_key": "EMPTY",
    "_api_key_note": "No authentication needed for local server",
    "model_name": "custom-model",
    "_model_name_note": "Descriptive name for your model (not used for file lookup with model_dir)"
  },

  "_paths_comment": "===== Storage Directories =====",
  "model_dir": "models",
  "_model_dir_note": "Base directory where all models are stored",
  "cache_dir": ".cache",
  "_cache_dir_note": "Cache directory for temporary files",

  "_logging_comment": "===== Logging Configuration =====",
  "_logging_levels": "Options: DEBUG, INFO, WARNING, ERROR, CRITICAL",
  "log_level": "ERROR",

  "_inference_params_comment": "===== Inference Parameters =====",
  "_inference_params_note": "These control how the LLM generates responses",
  "inference_params": {
    "temperature": 0.7,
    "_temperature_note": "Randomness (0.0 = deterministic, 2.0 = very creative)",
    "max_tokens": 2048,
    "_max_tokens_note": "Maximum response length in tokens",
    "top_p": 0.9,
    "_top_p_note": "Nucleus sampling (0.0-1.0, higher = more diversity)",
    "top_k": 50,
    "_top_k_note": "Top-k sampling (llama.cpp only, not supported by OpenAI)",
    "repetition_penalty": 1.1,
    "_repetition_penalty_note": "Penalty for repeating tokens (llama.cpp only, 1.0 = no penalty)"
  },

  "_switching_configs": {
    "to_huggingface": "To use HuggingFace structure: cp config.local.huggingface.example config.json",
    "to_openai": "To switch to OpenAI: cp config.openai.example config.json",
    "to_anthropic": "To switch to Anthropic: cp config.anthropic.example config.json"
  }
}
